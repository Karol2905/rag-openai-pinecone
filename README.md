
# Retrieval-Augmented Generation (RAG) with Pinecone and Gemini

## ğŸ“Œ Project Overview

This project implements a modern Retrieval-Augmented Generation (RAG) system using:

- HuggingFace Embeddings (all-MiniLM-L6-v2)
- Pinecone (vector database)
- Google Gemini (LLM)
- LangChain (LCEL architecture)

The system enhances LLM responses by retrieving relevant contextual information from a vector database before generating an answer.

---

## ğŸ§  What is RAG?

Retrieval-Augmented Generation (RAG) is an architecture that improves LLM performance by:

1. Retrieving relevant information from an external knowledge base.
2. Injecting retrieved context into the LLM.
3. Generating grounded and context-aware responses.

This reduces hallucinations and allows dynamic knowledge updates.

---

## ğŸ— Architecture

### Pipeline

1. Document Loading
2. Text Chunking
3. Embedding Generation (384-dimensional vectors)
4. Storage in Pinecone (Cosine Similarity)
5. Similarity Retrieval
6. Context-Aware Generation with Gemini

### Diagram

User â†’ Retriever (Pinecone) â†’ Retrieved Context â†’ Gemini LLM â†’ Response

---

## ğŸ›  Technologies Used

- Python 3.11
- LangChain (modern LCEL)
- HuggingFace Sentence Transformers
- Pinecone (Serverless)
- Google Gemini (gemini-1.5-flash)

---

## ğŸ“‚ Project Structure

````

rag-project/
â”‚
â”œâ”€â”€ ingest.py        # Index documents into Pinecone
â”œâ”€â”€ query.py         # Query the RAG system
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_document.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md

````

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone <repo_link>
cd rag-project
````

### 2ï¸âƒ£ Create virtual environment

```bash
python3.11 -m venv venv
source venv/bin/activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ” Environment Variables

Create a `.env` file:

```env
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=rag-hf
GOOGLE_API_KEY=your_google_api_key
```

---

## ğŸš€ Running the Project

### Step 1 â€“ Index documents

```bash
python ingest.py
```

This will:

* Load documents
* Split them into chunks
* Generate embeddings
* Store them in Pinecone

---

### Step 2 â€“ Query the system

```bash
python query.py
```

Example question:

```
What is RAG?
```

---

## ğŸ“Š Why Pinecone?

Pinecone enables efficient similarity search across high-dimensional embeddings, making retrieval scalable and fast.

---

## ğŸ“š Why HuggingFace Embeddings?

We use `all-MiniLM-L6-v2` because:

* It generates 384-dimensional vectors
* It is lightweight and efficient
* It requires no API cost

---

## ğŸ¤– Why Gemini?

Gemini generates context-aware responses based on retrieved documents, improving accuracy compared to standalone LLMs.

---

## ğŸ¯ Key Learning Outcomes

* Understanding semantic embeddings
* Vector similarity search
* Vector databases
* Retrieval-Augmented Generation
* Modern LangChain architecture (LCEL)

---

## ğŸ“Œ Conclusion

This project demonstrates a complete end-to-end RAG pipeline integrating retrieval and generation to enhance LLM responses with external knowledge.
